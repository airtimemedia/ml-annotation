{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Schema Update Script (SAFE - Preserves Both Train and Test)\n",
    "\n",
    "This notebook transforms BOTH train and test splits without deleting either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n",
      "✓ Dataset: Cantina/dj-image-train-data-20251117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root .env\n",
    "load_dotenv(Path('../..') / '.env')\n",
    "\n",
    "# Configuration\n",
    "DATASET_REPO = \"Cantina/dj-image-train-data-20251117\"  # Update this to your dataset\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN not found in environment variables\")\n",
    "\n",
    "print(f\"✓ Environment configured\")\n",
    "print(f\"✓ Dataset: {DATASET_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BOTH Train and Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f5553907be4cf48c2ca0affb7269d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb17fee2f944da2aebbcb55bdcbaec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/7.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82791a5107f2460a877c919d76d12c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/842k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d126506e9e6f490ba39c5958c3adc782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560f706d3ebe4ef39da0046215ed9928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1642 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Available splits: ['train', 'test']\n",
      "  - train: 14821 rows\n",
      "  - test: 1642 rows\n",
      "\n",
      "Current schema (train):\n",
      "{'prompt_name': Value('string'), 'new_room_unified_format_input': Value('string'), 'unified_format_output_enriched_fixed': Value('string'), 'gpt5-results-20250905': Value('string'), 'gpt5-results-20251104': Value('string'), 'last_updated_ts': Value('string')}\n",
      "\n",
      "First row sample (train):\n",
      "{'prompt_name': 'mention_not_in_history_prompt', 'new_room_unified_format_input': 'ROOM MEMBERS:[\\n  {\"user_name\": \"dj-marley\", \"full_name\": \"Marley\"},\\n  {\"user_name\": \"Lucas\", \"full_name\": \"Lucas\"},\\n  {\"user_name\": \"Olivia\", \"full_name\": \"Olivia\"},\\n  {\"user_name\": \"dj-aria\", \"full_name\": \"Aria\"}\\n]\\n\\nCHAT HISTORY:\\n\\n\\ndj-marley (bot): Just wrapped up my blues-rock playlist, lots of soulful guitar tonight. Hope everyone vibed with it!\\ndj-marley (bot): That Stevie Ray Vaughan track always hits different, especially late at night.\\nLucas: Totally agree, Marley, SRV\\'s solos are next-level. Always makes me nostalgic.\\ndj-marley (bot): For real, Lucas! \"Texas Flood\" is pure art. Glad you liked it!\\nOlivia: You should throw in some Hendrix next time, Marley. His riffs are legendary.\\ndj-marley (bot): Noted, Olivia! Next session we\\'ll dive into Jimi Hendrix classics. Can\\'t skip the legend.\\nLucas: Loving this vibe, Marley. But feeling like switching things up now.\\n\\n\\nLAST MESSAGE:\\nLucas: Hey @dj-aria, could you spin something electronic and upbeat, maybe some Daft Punk vibes?\\n\\nOUTPUT:', 'unified_format_output_enriched_fixed': '{\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\": \"an energetic electronic set with an upbeat feel and Daft Punk-inspired grooves\"}}', 'gpt5-results-20250905': '{\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\": \"electronic and upbeat playlist with Daft Punk vibes\"}}', 'gpt5-results-20251104': '{\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\": \"electronic and upbeat playlist with Daft Punk vibes\"}}', 'last_updated_ts': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset_dict = load_dataset(\n",
    "    DATASET_REPO,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Available splits: {list(dataset_dict.keys())}\")\n",
    "for split_name in dataset_dict.keys():\n",
    "    print(f\"  - {split_name}: {len(dataset_dict[split_name])} rows\")\n",
    "\n",
    "print(f\"\\nCurrent schema (train):\")\n",
    "print(dataset_dict['train'].features)\n",
    "\n",
    "print(f\"\\nFirst row sample (train):\")\n",
    "print(dataset_dict['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Schema for BOTH Splits\n",
    "\n",
    "Applies the same transformation to train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming train split...\n",
      "  Renaming columns...\n",
      "    ✓ Renamed: new_room_unified_format_input -> input\n",
      "    ✓ Renamed: gpt5-results-20251104 -> output\n",
      "  Removing columns...\n",
      "    ✓ Removed: unified_format_output_enriched_fixed, gpt5-results-20250905\n",
      "  Adding annotation columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfedc47aad7414dbf1a5860f482890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Added: manually_reviewed (bool)\n",
      "    ✓ Added: manually_reviewed_ts (int64)\n",
      "    ✓ Added: last_updated_ts (string)\n",
      "  ✓ train transformation complete!\n",
      "\n",
      "Transforming test split...\n",
      "  Renaming columns...\n",
      "    ✓ Renamed: new_room_unified_format_input -> input\n",
      "    ✓ Renamed: gpt5-results-20251104 -> output\n",
      "  Removing columns...\n",
      "    ✓ Removed: unified_format_output_enriched_fixed, gpt5-results-20250905\n",
      "  Adding annotation columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9479952a0c4228a2ff9362351fa4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1642 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Added: manually_reviewed (bool)\n",
      "    ✓ Added: manually_reviewed_ts (int64)\n",
      "    ✓ Added: last_updated_ts (string)\n",
      "  ✓ test transformation complete!\n",
      "\n",
      "✓ All splits transformed!\n",
      "\n",
      "Final schema:\n",
      "{'prompt_name': Value('string'), 'input': Value('string'), 'output': Value('string'), 'last_updated_ts': Value('string'), 'manually_reviewed': Value('bool'), 'manually_reviewed_ts': Value('int64')}\n",
      "\n",
      "Final split sizes:\n",
      "  - train: 14821 rows\n",
      "  - test: 1642 rows\n"
     ]
    }
   ],
   "source": [
    "def transform_split(dataset, split_name):\n",
    "    \"\"\"Transform a single split\"\"\"\n",
    "    print(f\"\\nTransforming {split_name} split...\")\n",
    "\n",
    "    # Step 1: Rename columns\n",
    "    print(\"  Renaming columns...\")\n",
    "    dataset = dataset.rename_column(\"new_room_unified_format_input\", \"input\")\n",
    "    dataset = dataset.rename_column(\"gpt5-results-20251104\", \"output\")\n",
    "    print(\"    ✓ Renamed: new_room_unified_format_input -> input\")\n",
    "    print(\"    ✓ Renamed: gpt5-results-20251104 -> output\")\n",
    "\n",
    "    # Step 2: Remove unwanted columns\n",
    "    print(\"  Removing columns...\")\n",
    "    columns_to_remove = [\"unified_format_output_enriched_fixed\", \"gpt5-results-20250905\"]\n",
    "    dataset = dataset.remove_columns(columns_to_remove)\n",
    "    print(f\"    ✓ Removed: {', '.join(columns_to_remove)}\")\n",
    "\n",
    "    # Step 3: Add annotation columns\n",
    "    print(\"  Adding annotation columns...\")\n",
    "    def add_annotation_columns(example):\n",
    "        example['manually_reviewed'] = False\n",
    "        example['manually_reviewed_ts'] = 0\n",
    "        example['last_updated_ts'] = ''\n",
    "        return example\n",
    "\n",
    "    dataset = dataset.map(add_annotation_columns)\n",
    "    print(\"    ✓ Added: manually_reviewed (bool)\")\n",
    "    print(\"    ✓ Added: manually_reviewed_ts (int64)\")\n",
    "    print(\"    ✓ Added: last_updated_ts (string)\")\n",
    "\n",
    "    print(f\"  ✓ {split_name} transformation complete!\")\n",
    "    return dataset\n",
    "\n",
    "# Transform each split\n",
    "transformed_dict = {}\n",
    "for split_name in dataset_dict.keys():\n",
    "    transformed_dict[split_name] = transform_split(dataset_dict[split_name], split_name)\n",
    "\n",
    "# Create new DatasetDict with both splits\n",
    "final_dataset = DatasetDict(transformed_dict)\n",
    "\n",
    "print(f\"\\n✓ All splits transformed!\")\n",
    "print(f\"\\nFinal schema:\")\n",
    "print(final_dataset['train'].features)\n",
    "print(f\"\\nFinal split sizes:\")\n",
    "for split_name in final_dataset.keys():\n",
    "    print(f\"  - {split_name}: {len(final_dataset[split_name])} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:\n",
      "\n",
      "TRAIN split:\n",
      "  Rows: 14821\n",
      "  Columns: ['prompt_name', 'input', 'output', 'last_updated_ts', 'manually_reviewed', 'manually_reviewed_ts']\n",
      "\n",
      "  Sample row:\n",
      "    prompt_name: mention_not_in_history_prompt\n",
      "    input: ROOM MEMBERS:[\n",
      "  {\"user_name\": \"dj-marley\", \"full_name\": \"Marley\"},\n",
      "  {\"user_name\": \"Lucas\", \"full_n...\n",
      "    output: {\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\":...\n",
      "    manually_reviewed: False\n",
      "    manually_reviewed_ts: 0\n",
      "    last_updated_ts: \n",
      "\n",
      "TEST split:\n",
      "  Rows: 1642\n",
      "  Columns: ['prompt_name', 'input', 'output', 'last_updated_ts', 'manually_reviewed', 'manually_reviewed_ts']\n",
      "\n",
      "  Sample row:\n",
      "    prompt_name: confirming_music_prompt\n",
      "    input: ROOM MEMBERS:[\n",
      "  {\"user_name\": \"Lucas\", \"full_name\": \"Lucas\"},\n",
      "  {\"user_name\": \"Jade\", \"full_name\": ...\n",
      "    output: {\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"Jade\"], \"action_metadata\": {\"prompt\": \"l...\n",
      "    manually_reviewed: False\n",
      "    manually_reviewed_ts: 0\n",
      "    last_updated_ts: \n"
     ]
    }
   ],
   "source": [
    "print(\"Validation:\")\n",
    "for split_name in final_dataset.keys():\n",
    "    print(f\"\\n{split_name.upper()} split:\")\n",
    "    print(f\"  Rows: {len(final_dataset[split_name])}\")\n",
    "    print(f\"  Columns: {final_dataset[split_name].column_names}\")\n",
    "\n",
    "    print(f\"\\n  Sample row:\")\n",
    "    sample = final_dataset[split_name][0]\n",
    "    for key in ['prompt_name', 'input', 'output', 'manually_reviewed', 'manually_reviewed_ts', 'last_updated_ts']:\n",
    "        if key in sample:\n",
    "            value = sample[key]\n",
    "            if isinstance(value, str) and len(value) > 100:\n",
    "                value = value[:100] + \"...\"\n",
    "            print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hugging Face\n",
    "\n",
    "⚠️ **IMPORTANT**: This will upload BOTH train and test splits with the new schema.\n",
    "Make sure you're happy with the transformation before running this cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing to Hugging Face...\n",
      "  Repository: Cantina/dj-image-train-data-20251117\n",
      "  train: 14821 rows\n",
      "  test: 1642 rows\n",
      "  Columns: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8745eb22d3e4867aa9a020a841ce5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252e862b058f4475af503ca5ed3f495f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f50e751c8154b1e9343c987d344bc11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cfd9fcbf864993b7ff1a9b263a4ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa3e8a6684942d7a9db060bbda43766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b940a57937446b873238f5963bbb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059eaabd7302461faf5b662cf8d770a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99c8713963f421699e653fbb6783ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully pushed BOTH splits!\n",
      "\n",
      "View at: https://huggingface.co/datasets/Cantina/dj-image-train-data-20251117\n"
     ]
    }
   ],
   "source": [
    "print(\"Pushing to Hugging Face...\")\n",
    "print(f\"  Repository: {DATASET_REPO}\")\n",
    "for split_name in final_dataset.keys():\n",
    "    print(f\"  {split_name}: {len(final_dataset[split_name])} rows\")\n",
    "print(f\"  Columns: {len(final_dataset['train'].column_names)}\")\n",
    "\n",
    "# Push the entire DatasetDict (includes all splits)\n",
    "final_dataset.push_to_hub(\n",
    "    DATASET_REPO,\n",
    "    token=HF_TOKEN,\n",
    "    commit_message=\"Transform dataset: rename columns, remove old ones, add annotation fields (both train and test)\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Successfully pushed BOTH splits!\")\n",
    "print(f\"\\nView at: https://huggingface.co/datasets/{DATASET_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
