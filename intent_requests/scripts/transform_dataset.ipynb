{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Schema Update Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n",
      "✓ Dataset: Cantina/intent-full-data-20251106\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root .env\n",
    "load_dotenv(Path('../..') / '.env')\n",
    "\n",
    "# Configuration\n",
    "DATASET_REPO = \"Cantina/intent-full-data-20251106\"\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN not found in environment variables\")\n",
    "\n",
    "print(f\"✓ Environment configured\")\n",
    "print(f\"✓ Dataset: {DATASET_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Loading dataset from Hugging Face...\")\ndataset = load_dataset(\n    DATASET_REPO,\n    split=\"train\",\n    token=HF_TOKEN\n)\n\nprint(f\"✓ Loaded {len(dataset)} rows\")\nprint(f\"\\nCurrent schema:\")\nprint(dataset.features)\n\nprint(f\"\\nFirst row sample:\")\nprint(dataset[0])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataset Schema\n",
    "\n",
    "Rename columns, remove old ones, and add annotation tracking fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Rename columns\nprint(\"Renaming columns...\")\ndataset = dataset.rename_column(\"new_room_unified_format_input\", \"input\")\ndataset = dataset.rename_column(\"gpt5-results-20251104\", \"output\")\nprint(\"✓ Renamed: new_room_unified_format_input -> input\")\nprint(\"✓ Renamed: gpt5-results-20251104 -> output\")\n\n# Step 2: Remove unwanted columns\nprint(\"\\nRemoving columns...\")\ncolumns_to_remove = [\"unified_format_output_enriched_fixed\", \"gpt5-results-20250905\"]\ndataset = dataset.remove_columns(columns_to_remove)\nprint(f\"✓ Removed: {', '.join(columns_to_remove)}\")\n\n# Step 3: Add annotation columns\nprint(\"\\nAdding annotation columns...\")\ndef add_annotation_columns(example):\n    example['manually_reviewed'] = False\n    example['manually_reviewed_ts'] = 0\n    example['last_updated_ts'] = ''\n    return example\n\ndataset = dataset.map(add_annotation_columns)\nprint(\"✓ Added: manually_reviewed (bool)\")\nprint(\"✓ Added: manually_reviewed_ts (int64)\")\nprint(\"✓ Added: last_updated_ts (string)\")\n\nprint(f\"\\n✓ Transformation complete!\")\nprint(f\"\\nFinal schema:\")\nprint(dataset.features)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Final columns:\")\nfor col_name in dataset.column_names:\n    print(f\"  ✓ {col_name}\")\n\nprint(f\"\\nTotal rows: {len(dataset)}\")\n\nprint(\"\\nSample row:\")\nsample = dataset[0]\nfor key in ['prompt_name', 'input', 'output', 'manually_reviewed', 'manually_reviewed_ts', 'last_updated_ts']:\n    if key in sample:\n        value = sample[key]\n        if isinstance(value, str) and len(value) > 100:\n            value = value[:100] + \"...\"\n        print(f\"  {key}: {value}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Repository\n",
    "\n",
    "Delete all existing files (except .gitattributes) to ensure a clean schema update."
   ]
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import HfApi, CommitOperationDelete\n\napi = HfApi()\n\nprint(\"Performing complete cleanup of repository...\")\ntry:\n    # List all files in the repo\n    repo_files = api.list_repo_files(DATASET_REPO, repo_type=\"dataset\", token=HF_TOKEN)\n    \n    print(f\"Files in repo: {len(repo_files)} files\")\n    \n    # Files to delete - everything except .gitattributes\n    files_to_delete = [f for f in repo_files if f != '.gitattributes']\n    \n    if files_to_delete:\n        print(f\"\\nDeleting {len(files_to_delete)} files:\")\n        for file in files_to_delete:\n            print(f\"  - {file}\")\n        \n        # Delete all files in one commit\n        operations = [CommitOperationDelete(path_in_repo=file) for file in files_to_delete]\n        \n        api.create_commit(\n            repo_id=DATASET_REPO,\n            repo_type=\"dataset\",\n            operations=operations,\n            token=HF_TOKEN,\n            commit_message=\"Delete all files before schema update\"\n        )\n        \n        print(\"\\n✓ All files deleted!\")\n    else:\n        print(\"No files to delete\")\n        \nexcept Exception as e:\n    print(f\"Error during cleanup: {e}\")\n    import traceback\n    traceback.print_exc()\n    raise",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Push to Hugging Face\n\nUpload the transformed dataset with the new schema.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "print(\"Pushing to Hugging Face...\")\nprint(f\"  Repository: {DATASET_REPO}\")\nprint(f\"  Rows: {len(dataset)}\")\nprint(f\"  Columns: {len(dataset.column_names)}\")\n\ndataset.push_to_hub(\n    DATASET_REPO,\n    token=HF_TOKEN,\n    commit_message=\"Transform dataset: rename columns, remove old ones, add annotation fields\"\n)\n\nprint(f\"\\n✓ Successfully pushed!\")\nprint(f\"\\nView at: https://huggingface.co/datasets/{DATASET_REPO}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}