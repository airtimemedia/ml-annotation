{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Schema Update Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Environment configured\n",
      "✓ Dataset: Cantina/dj-image-train-data-20251105_v3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from root .env\n",
    "load_dotenv(Path('../..') / '.env')\n",
    "\n",
    "# Configuration\n",
    "DATASET_REPO = \"Cantina/dj-image-train-data-20251105_v3\"\n",
    "HF_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
    "\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"HUGGINGFACE_TOKEN not found in environment variables\")\n",
    "\n",
    "print(f\"✓ Environment configured\")\n",
    "print(f\"✓ Dataset: {DATASET_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f22ef06562473c9492186d8aa6238f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd7db27374841d8a2d62b5fc7cde069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/8.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb97386e0dc24f37ae98ce702043210f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/877k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3581d6721a464aa3763ccc57468688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c863997e446f42418755e32441a3436a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1670 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 15023 rows\n",
      "\n",
      "Current schema:\n",
      "{'prompt_name': Value('string'), 'new_room_unified_format_input': Value('string'), 'unified_format_output_enriched_fixed': Value('string'), 'gpt5-results-20250905': Value('string'), 'gpt5-results-20251104': Value('string')}\n",
      "\n",
      "First row sample:\n",
      "{'prompt_name': 'mention_not_in_history_prompt', 'new_room_unified_format_input': 'ROOM MEMBERS:[\\n  {\"user_name\": \"dj-marley\", \"full_name\": \"Marley\"},\\n  {\"user_name\": \"Lucas\", \"full_name\": \"Lucas\"},\\n  {\"user_name\": \"Olivia\", \"full_name\": \"Olivia\"},\\n  {\"user_name\": \"dj-aria\", \"full_name\": \"Aria\"}\\n]\\n\\nCHAT HISTORY:\\n\\n\\ndj-marley (bot): Just wrapped up my blues-rock playlist, lots of soulful guitar tonight. Hope everyone vibed with it!\\ndj-marley (bot): That Stevie Ray Vaughan track always hits different, especially late at night.\\nLucas: Totally agree, Marley, SRV\\'s solos are next-level. Always makes me nostalgic.\\ndj-marley (bot): For real, Lucas! \"Texas Flood\" is pure art. Glad you liked it!\\nOlivia: You should throw in some Hendrix next time, Marley. His riffs are legendary.\\ndj-marley (bot): Noted, Olivia! Next session we\\'ll dive into Jimi Hendrix classics. Can\\'t skip the legend.\\nLucas: Loving this vibe, Marley. But feeling like switching things up now.\\n\\n\\nLAST MESSAGE:\\nLucas: Hey @dj-aria, could you spin something electronic and upbeat, maybe some Daft Punk vibes?\\n\\nOUTPUT:', 'unified_format_output_enriched_fixed': '{\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\": \"an energetic electronic set with an upbeat feel and Daft Punk-inspired grooves\"}}', 'gpt5-results-20250905': '{\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\": \"electronic and upbeat playlist with Daft Punk vibes\"}}', 'gpt5-results-20251104': '{\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\": \"electronic and upbeat playlist with Daft Punk vibes\"}}'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset from Hugging Face...\")\n",
    "dataset = load_dataset(\n",
    "    DATASET_REPO,\n",
    "    split=\"train\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded {len(dataset)} rows\")\n",
    "print(f\"\\nCurrent schema:\")\n",
    "print(dataset.features)\n",
    "\n",
    "print(f\"\\nFirst row sample:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dataset Schema\n",
    "\n",
    "Rename columns, remove old ones, and add annotation tracking fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming columns...\n",
      "✓ Renamed: new_room_unified_format_input -> input\n",
      "✓ Renamed: gpt5-results-20251104 -> output\n",
      "\n",
      "Removing columns...\n",
      "✓ Removed: unified_format_output_enriched_fixed, gpt5-results-20250905\n",
      "\n",
      "Adding annotation columns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27639014d68c43c8a2eddcb1adb7f5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Added: manually_reviewed (bool)\n",
      "✓ Added: manually_reviewed_ts (int64)\n",
      "✓ Added: last_updated_ts (string)\n",
      "\n",
      "✓ Transformation complete!\n",
      "\n",
      "Final schema:\n",
      "{'prompt_name': Value('string'), 'input': Value('string'), 'output': Value('string'), 'manually_reviewed': Value('bool'), 'manually_reviewed_ts': Value('int64'), 'last_updated_ts': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Rename columns\n",
    "print(\"Renaming columns...\")\n",
    "dataset = dataset.rename_column(\"new_room_unified_format_input\", \"input\")\n",
    "dataset = dataset.rename_column(\"gpt5-results-20251104\", \"output\")\n",
    "print(\"✓ Renamed: new_room_unified_format_input -> input\")\n",
    "print(\"✓ Renamed: gpt5-results-20251104 -> output\")\n",
    "\n",
    "# Step 2: Remove unwanted columns\n",
    "print(\"\\nRemoving columns...\")\n",
    "columns_to_remove = [\"unified_format_output_enriched_fixed\", \"gpt5-results-20250905\"]\n",
    "dataset = dataset.remove_columns(columns_to_remove)\n",
    "print(f\"✓ Removed: {', '.join(columns_to_remove)}\")\n",
    "\n",
    "# Step 3: Add annotation columns\n",
    "print(\"\\nAdding annotation columns...\")\n",
    "def add_annotation_columns(example):\n",
    "    example['manually_reviewed'] = False\n",
    "    example['manually_reviewed_ts'] = 0\n",
    "    example['last_updated_ts'] = ''\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(add_annotation_columns)\n",
    "print(\"✓ Added: manually_reviewed (bool)\")\n",
    "print(\"✓ Added: manually_reviewed_ts (int64)\")\n",
    "print(\"✓ Added: last_updated_ts (string)\")\n",
    "\n",
    "print(f\"\\n✓ Transformation complete!\")\n",
    "print(f\"\\nFinal schema:\")\n",
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Transformed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final columns:\n",
      "  ✓ prompt_name\n",
      "  ✓ input\n",
      "  ✓ output\n",
      "  ✓ manually_reviewed\n",
      "  ✓ manually_reviewed_ts\n",
      "  ✓ last_updated_ts\n",
      "\n",
      "Total rows: 15023\n",
      "\n",
      "Sample row:\n",
      "  prompt_name: mention_not_in_history_prompt\n",
      "  input: ROOM MEMBERS:[\n",
      "  {\"user_name\": \"dj-marley\", \"full_name\": \"Marley\"},\n",
      "  {\"user_name\": \"Lucas\", \"full_n...\n",
      "  output: {\"action\": \"dj\", \"requester\": \"Lucas\", \"requested_users\": [\"dj-aria\"], \"action_metadata\": {\"prompt\":...\n",
      "  manually_reviewed: False\n",
      "  manually_reviewed_ts: 0\n",
      "  last_updated_ts: \n"
     ]
    }
   ],
   "source": [
    "print(\"Final columns:\")\n",
    "for col_name in dataset.column_names:\n",
    "    print(f\"  ✓ {col_name}\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(dataset)}\")\n",
    "\n",
    "print(\"\\nSample row:\")\n",
    "sample = dataset[0]\n",
    "for key in ['prompt_name', 'input', 'output', 'manually_reviewed', 'manually_reviewed_ts', 'last_updated_ts']:\n",
    "    if key in sample:\n",
    "        value = sample[key]\n",
    "        if isinstance(value, str) and len(value) > 100:\n",
    "            value = value[:100] + \"...\"\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Repository\n",
    "\n",
    "Delete all existing files (except .gitattributes) to ensure a clean schema update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing complete cleanup of repository...\n",
      "Files in repo: 1 files\n",
      "No files to delete\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, CommitOperationDelete\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "print(\"Performing complete cleanup of repository...\")\n",
    "try:\n",
    "    # List all files in the repo\n",
    "    repo_files = api.list_repo_files(DATASET_REPO, repo_type=\"dataset\", token=HF_TOKEN)\n",
    "\n",
    "    print(f\"Files in repo: {len(repo_files)} files\")\n",
    "\n",
    "    # Files to delete - everything except .gitattributes\n",
    "    files_to_delete = [f for f in repo_files if f != '.gitattributes']\n",
    "\n",
    "    if files_to_delete:\n",
    "        print(f\"\\nDeleting {len(files_to_delete)} files:\")\n",
    "        for file in files_to_delete:\n",
    "            print(f\"  - {file}\")\n",
    "\n",
    "        # Delete all files in one commit\n",
    "        operations = [CommitOperationDelete(path_in_repo=file) for file in files_to_delete]\n",
    "\n",
    "        api.create_commit(\n",
    "            repo_id=DATASET_REPO,\n",
    "            repo_type=\"dataset\",\n",
    "            operations=operations,\n",
    "            token=HF_TOKEN,\n",
    "            commit_message=\"Delete all files before schema update\"\n",
    "        )\n",
    "\n",
    "        print(\"\\n✓ All files deleted!\")\n",
    "    else:\n",
    "        print(\"No files to delete\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during cleanup: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push to Hugging Face\n",
    "\n",
    "Upload the transformed dataset with the new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pushing to Hugging Face...\n",
      "  Repository: Cantina/dj-image-train-data-20251105_v3\n",
      "  Rows: 15023\n",
      "  Columns: 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ffc43ae879f47f3a4ae71b89abd2839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c352de2c51b477b93d54abe83cd1127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca96e7fd7f4843f1a49070a2993b13fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf9aa5b40b449b5b524af7d1089ebdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Successfully pushed!\n",
      "\n",
      "View at: https://huggingface.co/datasets/Cantina/dj-image-train-data-20251105_v3\n"
     ]
    }
   ],
   "source": [
    "print(\"Pushing to Hugging Face...\")\n",
    "print(f\"  Repository: {DATASET_REPO}\")\n",
    "print(f\"  Rows: {len(dataset)}\")\n",
    "print(f\"  Columns: {len(dataset.column_names)}\")\n",
    "\n",
    "dataset.push_to_hub(\n",
    "    DATASET_REPO,\n",
    "    token=HF_TOKEN,\n",
    "    commit_message=\"Transform dataset: rename columns, remove old ones, add annotation fields\"\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Successfully pushed!\")\n",
    "print(f\"\\nView at: https://huggingface.co/datasets/{DATASET_REPO}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
